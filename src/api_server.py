import os
import threading
import time
from flask import Flask, request, jsonify, render_template, Response
from flask_cors import CORS
from datetime import datetime
import git
from typing import Dict, Any, Optional
import subprocess
import json
import logging
import tempfile
import queue
import pickle
from pathlib import Path
import select
import fcntl

try:
    # Try relative imports (when running as module)
    from .remote_developer import RemoteDeveloper
    from .config import Config
    from .task_manager import task_manager
    from .database import db, save_task_to_db, get_task_from_db, add_log_to_db
except ImportError:
    # Fall back to absolute imports (when running directly)
    from remote_developer import RemoteDeveloper
    from config import Config
    from task_manager import task_manager
    from database import db, save_task_to_db, get_task_from_db, add_log_to_db

app = Flask(__name__, template_folder='../templates', static_folder='../static')
CORS(app)

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Initialize MongoDB connection
try:
    db.connect()
    logger.info("MongoDB connection initialized")
except Exception as e:
    logger.warning(f"MongoDB connection failed, using local storage: {e}")

# Create directory for task persistence
TASKS_DIR = Path.home() / '.remote_developer' / 'tasks'
TASKS_DIR.mkdir(parents=True, exist_ok=True)

# Store active tasks and their status
tasks_status = {}
tasks_lock = threading.Lock()

# Store log streams for real-time updates
log_streams = {}
log_streams_lock = threading.Lock()

def save_task_status(task_id: str):
    """Save task status to MongoDB and file (for backup)"""
    try:
        with tasks_lock:
            if task_id in tasks_status:
                # Save to MongoDB
                save_task_to_db(task_id, tasks_status[task_id])
                
                # Also save to file as backup
                task_file = TASKS_DIR / f"{task_id}.json"
                with open(task_file, 'w') as f:
                    json.dump(tasks_status[task_id], f, indent=2)
    except Exception as e:
        logger.error(f"Failed to save task {task_id}: {e}")

def load_all_tasks():
    """Load all tasks from MongoDB on startup"""
    try:
        # Try to load from MongoDB first
        all_tasks = db.get_all_tasks(limit=500)
        for task_data in all_tasks:
            task_id = task_data.get('task_id')
            if task_id:
                tasks_status[task_id] = task_data
                logger.info(f"Loaded task {task_id} from MongoDB")
        
        # Also check local files for any tasks not in MongoDB
        for task_file in TASKS_DIR.glob("*.json"):
            task_id = task_file.stem
            if task_id not in tasks_status:
                with open(task_file, 'r') as f:
                    task_data = json.load(f)
                    tasks_status[task_id] = task_data
                    # Save to MongoDB
                    save_task_to_db(task_id, task_data)
                    logger.info(f"Migrated task {task_id} from file to MongoDB")
    except Exception as e:
        logger.error(f"Failed to load tasks: {e}")

# Load existing tasks on startup
load_all_tasks()

def create_or_get_devpod(devpod_name: str) -> bool:
    """Create devpod if it doesn't exist"""
    try:
        # Use full path to devpod
        devpod_path = '/Users/namsangboy/.local/bin/devpod'
        
        # First check if devpod is installed
        version_result = subprocess.run([devpod_path, 'version'], 
                                      capture_output=True, text=True)
        if version_result.returncode != 0:
            logger.error("DevPod is not accessible. Check installation at: /Users/namsangboy/.local/bin/devpod")
            return False
            
        # Check if devpod exists
        result = subprocess.run([devpod_path, 'list', '--output', 'json'], 
                              capture_output=True, text=True)
        
        if result.returncode != 0:
            logger.error(f"Failed to list devpods: {result.stderr}")
            # Try without --output json flag
            result = subprocess.run([devpod_path, 'list'], 
                                  capture_output=True, text=True)
            if result.returncode != 0:
                logger.error(f"Failed to list devpods: {result.stderr}")
                return False
            
            # Parse text output
            exists = devpod_name in result.stdout
        else:
            # Parse JSON output
            try:
                devpods = json.loads(result.stdout)
                exists = any(pod.get('Name') == devpod_name for pod in devpods)
            except json.JSONDecodeError:
                # Fallback to text parsing
                exists = devpod_name in result.stdout
            
        if not exists:
            logger.info(f"Creating new devpod: {devpod_name}")
            
            # List available providers
            providers_result = subprocess.run([devpod_path, 'provider', 'list'], 
                                            capture_output=True, text=True)
            logger.info(f"Available providers: {providers_result.stdout}")
            
            # Check kubernetes provider settings
            k8s_use_result = subprocess.run([devpod_path, 'provider', 'use', 'kubernetes'], 
                                           capture_output=True, text=True)
            logger.info(f"Setting kubernetes as provider: {k8s_use_result.stdout}")
            
            # Create local workspace directory
            workspace_dir = f"/tmp/devpod-workspace-{devpod_name}"
            os.makedirs(workspace_dir, exist_ok=True)
            
            # Create new devpod with kubernetes provider (default)
            create_result = subprocess.run([devpod_path, 'up', '--provider', 'kubernetes', f'--source', f'local:{workspace_dir}', devpod_name], 
                                         capture_output=True, text=True)
            if create_result.returncode != 0:
                logger.error(f"Failed to create devpod with kubernetes: {create_result.stderr}")
                # Try without specifying provider (use default)
                create_result = subprocess.run([devpod_path, 'up', f'--source', f'local:{workspace_dir}', devpod_name], 
                                             capture_output=True, text=True)
                if create_result.returncode != 0:
                    logger.error(f"Failed to create devpod: {create_result.stderr}")
                    return False
                    
        else:
            logger.info(f"DevPod {devpod_name} already exists")
            
        # Start the devpod if it's not running
        logger.info(f"Ensuring DevPod {devpod_name} is running...")
        start_result = subprocess.run([devpod_path, 'up', devpod_name], 
                                    capture_output=True, text=True)
        if start_result.returncode != 0:
            logger.warning(f"Failed to ensure devpod is running: {start_result.stderr}")
        else:
            logger.info(f"DevPod {devpod_name} is ready")
            
        return True
    except FileNotFoundError:
        logger.error("DevPod command not found. Please install DevPod: https://devpod.sh/docs/getting-started/install")
        return False
    except Exception as e:
        logger.error(f"Error managing devpod: {e}")
        return False

def add_log(task_id: str, message: str):
    """Add log message to MongoDB and notify streams"""
    # Save to MongoDB
    add_log_to_db(task_id, message)
    
    with tasks_lock:
        if task_id in tasks_status:
            # Keep recent logs in memory for quick access
            if 'logs' not in tasks_status[task_id]:
                tasks_status[task_id]['logs'] = []
            
            # Keep only last 100 logs in memory
            tasks_status[task_id]['logs'].append(message)
            if len(tasks_status[task_id]['logs']) > 100:
                tasks_status[task_id]['logs'] = tasks_status[task_id]['logs'][-100:]
            
            tasks_status[task_id]['last_updated'] = datetime.now().isoformat()
            
            # Auto-detect completion from log messages
            if any(keyword in message.lower() for keyword in ['completed!', 'finished!', 'all tasks finished']):
                if tasks_status[task_id]['status'] != 'completed':
                    logger.info(f"Auto-detected task completion for {task_id}")
            
            # Periodically save status for long-running tasks (every 10 logs)
            if len(tasks_status[task_id]['logs']) % 10 == 0:
                save_task_status(task_id)
    
    # Notify all streams watching this task
    with log_streams_lock:
        if task_id in log_streams:
            for stream_queue in log_streams[task_id]:
                try:
                    stream_queue.put(message)
                except:
                    pass

def parse_claude_output(task_id: str, line_text: str):
    """Parse Claude-specific output patterns for better progress tracking"""
    with tasks_lock:
        if task_id not in tasks_status:
            return
            
        # Parse various Claude output patterns
        if "CLAUDE_STATUS:" in line_text:
            if "RUNNING" in line_text and "minutes" in line_text:
                try:
                    minutes = int(line_text.split('(')[1].split(' ')[0])
                    tasks_status[task_id]['claude_runtime'] = minutes * 60
                except:
                    pass
            status = line_text.split("CLAUDE_STATUS:")[1].strip().split()[0]
            tasks_status[task_id]['claude_status'] = status
            tasks_status[task_id]['last_updated'] = datetime.now().isoformat()
            # save_task_status(task_id)  # Commented out for performance
            
        # Parse Claude progress indicators
        elif "â–ˆ" in line_text or "â–“" in line_text:  # Progress bar
            tasks_status[task_id]['claude_progress_bar'] = line_text
            
        # Parse file operations
        elif any(keyword in line_text.lower() for keyword in ['creating', 'writing', 'updating', 'modifying']):
            if 'claude_files_modified' not in tasks_status[task_id]:
                tasks_status[task_id]['claude_files_modified'] = []
            tasks_status[task_id]['claude_files_modified'].append(line_text)
            
        # Parse thinking/planning output
        elif any(keyword in line_text.lower() for keyword in ['thinking', 'planning', 'analyzing']):
            tasks_status[task_id]['claude_thinking'] = True
            
        # Parse completion indicators
        elif any(keyword in line_text.lower() for keyword in ['completed', 'finished', 'done', 'ì™„ë£Œ']):
            tasks_status[task_id]['claude_status'] = 'COMPLETED'
            tasks_status[task_id]['last_updated'] = datetime.now().isoformat()
            # save_task_status(task_id)  # Commented out for performance

def get_pod_name(devpod_name: str) -> str:
    """Get pod name for devpod with timeout"""
    logger.info(f"Getting pod name for devpod: {devpod_name}")
    try:
        # Method 1: Try with label selector (reduced timeout)
        pod_name_cmd = f"timeout 2 kubectl get pods -n devpod -l devpod.sh/workspace={devpod_name} -o jsonpath='{{.items[0].metadata.name}}'"
        logger.debug(f"Running: {pod_name_cmd}")
        pod_result = subprocess.run(pod_name_cmd, shell=True, capture_output=True, text=True)
        pod_name = pod_result.stdout.strip()
        logger.debug(f"Method 1 result: {pod_name}")
        
        # Method 2: If not found, try with name prefix
        if not pod_name:
            # DevPod names get truncated, so search by prefix
            pod_name_cmd = f"timeout 2 kubectl get pods -n devpod | grep -E 'devpod-{devpod_name[:10]}' | awk '{{print $1}}' | head -1"
            pod_result = subprocess.run(pod_name_cmd, shell=True, capture_output=True, text=True)
            pod_name = pod_result.stdout.strip()
            
            # Method 3: More flexible search
            if not pod_name:
                # Try even shorter prefix
                short_name = devpod_name.replace('-', '')[:10]
                pod_name_cmd = f"timeout 2 kubectl get pods -n devpod | grep -i '{short_name}' | awk '{{print $1}}' | head -1"
                pod_result = subprocess.run(pod_name_cmd, shell=True, capture_output=True, text=True)
                pod_name = pod_result.stdout.strip()
        
        if not pod_name:
            # List all devpod pods for debugging
            list_cmd = "timeout 2 kubectl get pods -n devpod"
            list_result = subprocess.run(list_cmd, shell=True, capture_output=True, text=True)
            logger.error(f"Available pods:\n{list_result.stdout}")
            raise Exception(f"Pod not found for devpod {devpod_name}")
        
        return pod_name
    except Exception as e:
        logger.error(f"Failed to get pod name for {devpod_name}: {e}")
        raise

def exec_in_devpod(devpod_name: str, command: str, pod_name: str = None) -> subprocess.CompletedProcess:
    """Execute command in devpod using kubectl exec"""
    if not pod_name:
        pod_name = get_pod_name(devpod_name)
    
    logger.info(f"Using pod: {pod_name}")
    
    # Escape single quotes in command
    escaped_cmd = command.replace("'", "'\"'\"'")
    # Add timeout to kubectl exec (increased for long operations)
    kubectl_cmd = f"timeout 120 kubectl exec -n devpod {pod_name} -- bash -c '{escaped_cmd}'"
    return subprocess.run(kubectl_cmd, shell=True, capture_output=True, text=True)

def exec_in_devpod_stream_realtime(devpod_name: str, command: str, task_id: str, pod_name: str = None):
    """Execute command in devpod with real-time streaming using non-blocking I/O"""
    if not pod_name:
        logger.warning(f"Pod name not provided for streaming execution, getting it now...")
        try:
            pod_name = get_pod_name(devpod_name)
        except Exception as e:
            logger.error(f"Failed to get pod name: {e}")
            add_log(task_id, f"Error: Failed to get pod name: {e}")
            return -1
    
    # Escape single quotes in command
    escaped_cmd = command.replace("'", "'\"'\"'")
    # Note: -t flag removed as it can cause issues with non-interactive scripts
    kubectl_cmd = f"kubectl exec -n devpod {pod_name} -- bash -c '{escaped_cmd}'"
    
    try:
        # Set environment variables to disable buffering
        env = os.environ.copy()
        env['PYTHONUNBUFFERED'] = '1'
        
        # Run the command with real-time output capture
        process = subprocess.Popen(
            kubectl_cmd, 
            shell=True, 
            stdout=subprocess.PIPE, 
            stderr=subprocess.STDOUT, 
            bufsize=0,  # Unbuffered
            env=env
        )
        
        # Make stdout non-blocking
        fd = process.stdout.fileno()
        fl = fcntl.fcntl(fd, fcntl.F_GETFL)
        fcntl.fcntl(fd, fcntl.F_SETFL, fl | os.O_NONBLOCK)
        
        output_buffer = b""
        while True:
            # Check if process has ended
            poll_status = process.poll()
            
            # Use select to check if data is available
            ready, _, _ = select.select([process.stdout], [], [], 0.1)
            
            if ready:
                try:
                    # Read available data
                    chunk = os.read(fd, 4096)
                    if chunk:
                        output_buffer += chunk
                        
                        # Process complete lines
                        while b'\n' in output_buffer:
                            line, output_buffer = output_buffer.split(b'\n', 1)
                            try:
                                line_text = line.decode('utf-8', errors='replace').rstrip()
                                add_log(task_id, line_text)
                                
                                # Parse Claude status updates
                                parse_claude_output(task_id, line_text)
                            except Exception as e:
                                logger.warning(f"Error decoding line: {e}")
                except OSError:
                    # No data available
                    pass
            
            # If process ended and no more data, break
            if poll_status is not None and not ready:
                # Process any remaining data
                if output_buffer:
                    try:
                        line_text = output_buffer.decode('utf-8', errors='replace').rstrip()
                        if line_text:
                            add_log(task_id, line_text)
                    except:
                        pass
                break
        
        return_code = process.wait()
        
        # Log process completion
        if return_code == 0:
            add_log(task_id, f"âœ… Real-time process completed successfully (exit code: {return_code})")
        else:
            add_log(task_id, f"âš ï¸ Real-time process finished with exit code: {return_code}")
            
        return return_code
        
    except Exception as e:
        logger.error(f"Error in real-time streaming execution: {e}")
        add_log(task_id, f"âŒ Streaming error: {e}")
        return -1

def exec_in_devpod_stream_simple(devpod_name: str, command: str, task_id: str, pod_name: str = None):
    """Execute command in devpod with simpler streaming output - similar to manual script"""
    if not pod_name:
        logger.warning(f"Pod name not provided for streaming execution, getting it now...")
        try:
            pod_name = get_pod_name(devpod_name)
        except Exception as e:
            logger.error(f"Failed to get pod name: {e}")
            add_log(task_id, f"Error: Failed to get pod name: {e}")
            return -1
    
    # Escape single quotes in command
    escaped_cmd = command.replace("'", "'\"'\"'")
    kubectl_cmd = f"kubectl exec -n devpod {pod_name} -- bash -c '{escaped_cmd}'"
    
    try:
        # Run the command with real-time output capture
        process = subprocess.Popen(
            kubectl_cmd, 
            shell=True, 
            stdout=subprocess.PIPE, 
            stderr=subprocess.STDOUT, 
            text=True,
            bufsize=1,  # Line buffered
            universal_newlines=True
        )
        
        # Read output line by line
        for line in iter(process.stdout.readline, ''):
            if line:
                line_text = line.rstrip()
                add_log(task_id, line_text)
                
                # Parse Claude status updates
                parse_claude_output(task_id, line_text)
        
        # Wait for process to complete
        return_code = process.wait()
        
        # Log completion immediately
        logger.info(f"Streaming completed for task {task_id} with return code {return_code}")
        
        return return_code
        
    except Exception as e:
        logger.error(f"Error in streaming execution: {e}")
        add_log(task_id, f"Error: {e}")
        return -1

def execute_remote_task(task_id: str, devpod_name: str, github_repo: str, 
                       github_token: str, task_description: str):
    """Execute task in background thread"""
    devpod_path = '/Users/namsangboy/.local/bin/devpod'
    task_completed = False
    try:
        with tasks_lock:
            tasks_status[task_id] = {
                'status': 'initializing',
                'progress': 0,
                'logs': [],
                'created_at': datetime.now().isoformat(),
                'last_updated': datetime.now().isoformat(),
                'devpod_name': devpod_name,
                'github_repo': github_repo,
                'task_description': task_description,
                # GitHub token is not stored for security reasons - managed by frontend
            }
        
        save_task_status(task_id)
        
        # Step 1: Create or get devpod
        with tasks_lock:
            tasks_status[task_id]['status'] = 'creating_devpod'
            tasks_status[task_id]['progress'] = 10
        save_task_status(task_id)
        add_log(task_id, 'Creating/checking devpod...')
        
        if not create_or_get_devpod(devpod_name):
            error_msg = "Failed to create or get devpod. Please ensure DevPod is installed: https://devpod.sh/docs/getting-started/install"
            add_log(task_id, f'Error: {error_msg}')
            raise Exception(error_msg)
        
        # Step 2: Clone or update repository
        with tasks_lock:
            tasks_status[task_id]['status'] = 'cloning_repository'
            tasks_status[task_id]['progress'] = 20
        add_log(task_id, 'Cloning/updating repository...')
        
        # Configure git in devpod
        git_config_cmds = [
            f'git config --global user.name "Auto Worker"',
            f'git config --global user.email "auto-worker@example.com"',
            f'git config --global credential.helper store',
            f'echo "https://{github_token}@github.com" > ~/.git-credentials'
        ]
        
        # Get pod name early
        try:
            pod_name = get_pod_name(devpod_name)
        except Exception as e:
            logger.error(f"Failed to get pod name: {e}")
            error_msg = f"Failed to connect to DevPod {devpod_name}. Please ensure the DevPod is running."
            add_log(task_id, f'Error: {error_msg}')
            with tasks_lock:
                tasks_status[task_id]['status'] = 'failed'
                tasks_status[task_id]['error'] = error_msg
            save_task_status(task_id)
            return
        
        for cmd in git_config_cmds:
            exec_in_devpod(devpod_name, cmd, pod_name)
        
        # Clone or pull latest code
        repo_name = github_repo.split('/')[-1]
        
        # Check if repository already exists
        check_repo_cmd = f'cd ~/{repo_name} && git status'
        check_result = exec_in_devpod(devpod_name, check_repo_cmd, pod_name)
        
        if check_result.returncode == 0:
            # Repository exists, update it
            add_log(task_id, 'Repository exists, updating...')
            
            update_cmds = [
                f'cd ~/{repo_name} && git fetch origin',
                f'cd ~/{repo_name} && git checkout main || git checkout master || git checkout -b main',
                f'cd ~/{repo_name} && git reset --hard origin/$(git symbolic-ref --short HEAD) || true',
                f'cd ~/{repo_name} && git clean -fd'
            ]
            
            for cmd in update_cmds:
                result = exec_in_devpod(devpod_name, cmd, pod_name)
                if "error" in result.stderr.lower() and "fatal" in result.stderr.lower():
                    logger.warning(f"Git command warning: {result.stderr}")
        else:
            # Repository doesn't exist, clone it
            add_log(task_id, 'Cloning repository...')
            
            clone_cmd = f'cd ~ && git clone https://{github_token}@github.com/{github_repo}.git {repo_name}'
            result = exec_in_devpod(devpod_name, clone_cmd, pod_name)
            
            if result.returncode != 0:
                # If clone fails, try to remove existing directory and clone again
                remove_cmd = f'rm -rf ~/{repo_name}'
                exec_in_devpod(devpod_name, remove_cmd, pod_name)
                
                result = exec_in_devpod(devpod_name, clone_cmd, pod_name)
                if result.returncode != 0:
                    raise Exception(f"Failed to clone repository: {result.stderr}")
        
        # Step 3: Setup Claude Code
        with tasks_lock:
            tasks_status[task_id]['status'] = 'setting_up_claude'
            tasks_status[task_id]['progress'] = 30
        add_log(task_id, 'Setting up Claude Code...')
        
        # Install Node.js if not present
        node_check = 'which node || echo "not found"'
        result = exec_in_devpod(devpod_name, node_check, pod_name)
        
        if "not found" in result.stdout:
            add_log(task_id, 'Installing Node.js...')
            
            # Install Node.js
            node_install_cmds = [
                'apt-get update',
                'apt-get install -y nodejs npm'
            ]
            for cmd in node_install_cmds:
                exec_in_devpod(devpod_name, cmd, pod_name)
        
        # Install Claude via npm
        claude_install = 'npm install -g @anthropic-ai/claude-code || echo "Claude installation failed"'
        result = exec_in_devpod(devpod_name, claude_install, pod_name)
        
        if "Claude installation failed" not in result.stdout:
            add_log(task_id, 'Claude installed (from @anthropic-ai/claude-code package)')
        else:
            add_log(task_id, 'Claude installation failed, will use fallback')
        
        # Create Claude settings file with permissions
        claude_settings = '''mkdir -p ~/.claude && cat > ~/.claude/settings.json << 'EOF'
{
  "permissions": {
    "allow": [
      "Write(*)",
      "Read(*)",
      "Edit(*)",
      "Bash(*)",
      "Grep(*)",
      "Glob(*)",
      "MultiEdit(*)"
    ],
    "deny": []
  },
  "enableAllProjectMcpServers": false
}
EOF'''
        exec_in_devpod(devpod_name, claude_settings, pod_name)
        add_log(task_id, 'Created Claude settings with auto-permissions')
        
        # The package installs 'claude' command, not 'claude-code'
        check_claude = 'which claude || echo "not found"'
        check_result = exec_in_devpod(devpod_name, check_claude, pod_name)
        
        add_log(task_id, f'Claude command location: {check_result.stdout.strip()}')
        
        # Step 4: Stay on main branch (no automatic branch creation)
        with tasks_lock:
            tasks_status[task_id]['status'] = 'preparing_workspace'
            tasks_status[task_id]['progress'] = 40
        add_log(task_id, 'Preparing workspace on main branch...')
        
        # Ensure we're on main/master branch
        checkout_cmd = f'cd ~/{repo_name} && (git checkout main || git checkout master)'
        exec_in_devpod(devpod_name, checkout_cmd, pod_name)
        
        # Get current branch name for reference
        get_branch_cmd = f'cd ~/{repo_name} && git branch --show-current'
        branch_result = exec_in_devpod(devpod_name, get_branch_cmd, pod_name)
        current_branch = branch_result.stdout.strip()
        add_log(task_id, f'Working on branch: {current_branch}')
        
        # Step 5: Execute Claude task
        with tasks_lock:
            tasks_status[task_id]['status'] = 'executing_task'
            tasks_status[task_id]['progress'] = 60
        add_log(task_id, 'Executing Claude task...')
        
        # Create a simpler script similar to manual_debug.success.sh
        claude_script = f'''#!/bin/bash
# Claude execution script
cd ~/{repo_name}

echo "=== Claude ì‹¤í–‰ ì‹œì‘ ==="
echo "Task: {task_description}"

if command -v claude &> /dev/null; then
    echo "Claude ëª…ë ¹ì–´ ìœ„ì¹˜: $(which claude)"
    
    # Claude execution with extended timeout for long-running tasks
    echo "Executing Claude (timeout: 2 hours)..."
    # 7200 seconds = 2 hours timeout for very long Claude tasks
    timeout 7200 claude --print "{task_description}" 2>&1 | tee claude_output.txt
    
    echo ""
    echo "=== Claude ì‹¤í–‰ ì™„ë£Œ ==="
else
    echo "Claudeë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤!"
    exit 1
fi

# ìƒì„±ëœ íŒŒì¼ í™•ì¸
echo ""
echo "=== ìƒì„±ëœ íŒŒì¼ ==="
ls -la *.py 2>/dev/null || echo "Python íŒŒì¼ ì—†ìŒ"

# Ensure script exits properly
echo "=== Script completed ==="
exit 0
'''
        
        # Write and execute the script
        # Use base64 encoding to avoid shell escaping issues
        import base64
        script_encoded = base64.b64encode(claude_script.encode()).decode()
        script_cmd = f'cd ~/{repo_name} && echo "{script_encoded}" | base64 -d > run_claude.sh && chmod +x run_claude.sh'
        exec_in_devpod(devpod_name, script_cmd, pod_name)
        
        # Execute Claude with simpler streaming
        add_log(task_id, 'Claude ì‹¤í–‰ ì¤‘...')
        
        # Track Claude execution status
        with tasks_lock:
            tasks_status[task_id]['claude_status'] = 'STARTING'
            tasks_status[task_id]['claude_runtime'] = 0
        
        # Execute Claude script with simpler streaming output
        execute_cmd = f'cd ~/{repo_name} && bash run_claude.sh 2>&1'
        
        # Use simpler streaming execution for better reliability
        add_log(task_id, "Starting Claude execution with streaming...")
        returncode = exec_in_devpod_stream_simple(devpod_name, execute_cmd, task_id, pod_name)
        
        # Small delay to ensure all output is captured
        time.sleep(1)
        
        add_log(task_id, f"Claude script execution finished with return code: {returncode}")
        
        # Check if Claude succeeded
        claude_failed = returncode != 0
        
        # Update Claude status based on execution result
        with tasks_lock:
            if claude_failed:
                tasks_status[task_id]['claude_status'] = 'FAILED'
            else:
                tasks_status[task_id]['claude_status'] = 'COMPLETED'
            tasks_status[task_id]['last_updated'] = datetime.now().isoformat()
        save_task_status(task_id)
        
        # Log the result after releasing the lock
        if claude_failed:
            add_log(task_id, f"Claude execution failed with return code: {returncode}")
        else:
            add_log(task_id, "Claude execution completed successfully")
        
        # If Claude fails or is not installed, use fallback
        if claude_failed:
            add_log(task_id, "Using fallback implementation...")
            
            # Fallback: Create files based on task description
            if "streamlit" in task_description.lower():
                # Extract specific requirements from task description
                if "íŒŒë€ìƒ‰" in task_description and "4ë²ˆ" in task_description:
                    # Create app according to specific requirements
                    fallback_script = '''#!/bin/bash
cd ~/auto-worker-demo

# Create Streamlit app with specific requirements
cat > app.py << 'EOF'
import streamlit as st

st.title("ë¬¸ìì—´ ì¶œë ¥ ë°ëª¨")
st.write("ë¬¸ìì—´ì„ ì…ë ¥í•˜ë©´ íŒŒë€ìƒ‰ìœ¼ë¡œ 4ë²ˆ ì¶œë ¥í•©ë‹ˆë‹¤")

# ì‚¬ìš©ìë¡œë¶€í„° ë¬¸ìì—´ ì…ë ¥ ë°›ê¸°
user_input = st.text_input("ë¬¸ìì—´ì„ ì…ë ¥í•˜ì„¸ìš”:", "")

# ì…ë ¥ëœ ë¬¸ìì—´ì„ íŒŒë€ìƒ‰ìœ¼ë¡œ 4ë²ˆ ì¶œë ¥
if user_input:
    st.write("---")
    for i in range(4):
        st.markdown(f'<p style="color:blue;">{user_input}</p>', unsafe_allow_html=True)
    st.write("---")
EOF

# Create requirements.txt
echo "streamlit" > requirements.txt

echo "Streamlit app created successfully!"
ls -la
'''
                else:
                    # Default fallback
                    fallback_script = '''#!/bin/bash
cd ~/auto-worker-demo

# Create Streamlit app
cat > app.py << 'EOF'
import streamlit as st

st.title("Text Display Demo")
st.write("This application displays text input from users")

# Input text from user
user_input = st.text_input("Enter your text here:", "")

# Display the input text
if user_input:
    st.write("You entered:")
    st.success(user_input)
    
    # Additional display options
    with st.expander("View in different formats"):
        st.code(user_input)
        st.info(user_input)
        st.markdown(f"**Bold:** **{user_input}**")

# Add some statistics
if user_input:
    st.sidebar.header("Text Statistics")
    st.sidebar.write(f"Length: {len(user_input)} characters")
    st.sidebar.write(f"Words: {len(user_input.split())} words")
EOF

# Create requirements.txt
echo "streamlit" > requirements.txt

echo "Streamlit app created successfully!"
ls -la
'''
                # Use base64 encoding to avoid shell escaping issues
                fallback_encoded = base64.b64encode(fallback_script.encode()).decode()
                fallback_cmd = f'cd ~/{repo_name} && echo "{fallback_encoded}" | base64 -d > fallback.sh && chmod +x fallback.sh && bash fallback.sh'
                result = exec_in_devpod(devpod_name, fallback_cmd, pod_name)
                
                add_log(task_id, "Fallback execution completed")
                # Update status after fallback
                with tasks_lock:
                    tasks_status[task_id]['claude_status'] = 'FALLBACK_COMPLETED'
                    tasks_status[task_id]['last_updated'] = datetime.now().isoformat()
                save_task_status(task_id)
        
        # Step 7: Show modified files (no automatic commit)
        add_log(task_id, "=== Checking modified files ===")
        with tasks_lock:
            tasks_status[task_id]['status'] = 'reviewing_changes'
            tasks_status[task_id]['progress'] = 75
        save_task_status(task_id)  # Save status at this important transition
        
        # Show git status
        status_cmd = f'cd ~/{repo_name} && git status --short'
        status_result = exec_in_devpod(devpod_name, status_cmd, pod_name)
        if status_result.stdout.strip():
            add_log(task_id, 'Modified files:')
            for line in status_result.stdout.strip().split('\n'):
                add_log(task_id, f'  {line}')
            
            # Store modified files info
            with tasks_lock:
                tasks_status[task_id]['modified_files'] = status_result.stdout.strip().split('\n')
                tasks_status[task_id]['has_changes'] = True
        else:
            add_log(task_id, 'No files were modified')
            with tasks_lock:
                tasks_status[task_id]['has_changes'] = False
        
        # Step 8: Check if a server was created and run it
        add_log(task_id, "=== Moving to server check phase ===")
        with tasks_lock:
            tasks_status[task_id]['status'] = 'checking_server'
            tasks_status[task_id]['progress'] = 85
        save_task_status(task_id)  # Save status at this important transition
        add_log(task_id, 'Checking for server applications...')
        
        # Store server info
        server_started = False
        
        # Check for Python requirements
        req_check = exec_in_devpod(devpod_name, f'cd ~/{repo_name} && [ -f requirements.txt ] && echo "FOUND"', pod_name)
        if "FOUND" in req_check.stdout:
            add_log(task_id, 'Installing Python dependencies...')
            install_result = exec_in_devpod(devpod_name, f'cd ~/{repo_name} && pip3 install --break-system-packages -r requirements.txt || true', pod_name)
            add_log(task_id, f'Dependencies installation completed (return code: {install_result.returncode})')
        
        # Check for Streamlit app
        streamlit_check = exec_in_devpod(devpod_name, f'cd ~/{repo_name} && ([ -f app.py ] || [ -f streamlit_app.py ]) && grep -l "streamlit" *.py 2>/dev/null | head -1', pod_name)
        if streamlit_check.stdout.strip():
            app_file = streamlit_check.stdout.strip()
            add_log(task_id, f'Starting Streamlit app: {app_file}')
            
            # Kill any existing streamlit process
            exec_in_devpod(devpod_name, 'pkill -f streamlit || true', pod_name)
            
            # Start Streamlit in background
            start_cmd = f'cd ~/{repo_name} && nohup streamlit run {app_file} --server.port 8501 --server.address 0.0.0.0 > streamlit.log 2>&1 &'
            exec_in_devpod(devpod_name, start_cmd, pod_name)
            
            # Wait a bit for server to start
            time.sleep(3)
            
            # Set up port forwarding
            add_log(task_id, 'Setting up port forwarding for Streamlit (port 8501)...')
            forward_thread = threading.Thread(
                target=lambda: subprocess.run(['kubectl', 'port-forward', '-n', 'devpod', pod_name, '8501:8501'])
            )
            forward_thread.daemon = True
            forward_thread.start()
            
            add_log(task_id, 'âœ… Streamlit app is running!')
            add_log(task_id, 'ğŸŒ Access your app at: http://localhost:8501')
            
            with tasks_lock:
                tasks_status[task_id]['app_url'] = 'http://localhost:8501'
                tasks_status[task_id]['server_type'] = 'streamlit'
            
            server_started = True
        
        # Complete - even if server is running
        add_log(task_id, "=== COMPLETING TASK ===")
        with tasks_lock:
            tasks_status[task_id]['status'] = 'completed'
            tasks_status[task_id]['progress'] = 100
            tasks_status[task_id]['current_branch'] = current_branch
            tasks_status[task_id]['last_updated'] = datetime.now().isoformat()
            if server_started:
                tasks_status[task_id]['server_running'] = True
        save_task_status(task_id)
        add_log(task_id, f"Task status saved as 'completed' with progress 100%")
        
        if server_started:
            add_log(task_id, f'âœ… Task completed! Server is running on branch: {current_branch}')
        else:
            add_log(task_id, f'âœ… Task completed! Changes are ready on branch: {current_branch}')
            
        # Final completion log
        add_log(task_id, f'ğŸ‰ All tasks finished! Status: completed')
        task_completed = True
            
    except Exception as e:
        logger.error(f"Task {task_id} failed with exception: {e}")
        with tasks_lock:
            tasks_status[task_id]['status'] = 'failed'
            tasks_status[task_id]['error'] = str(e)
            tasks_status[task_id]['last_updated'] = datetime.now().isoformat()
        save_task_status(task_id)
        add_log(task_id, f'âŒ Error: {str(e)}')
        add_log(task_id, f'ğŸ’¥ Task failed! Status: failed')
    finally:
        # Ensure task status is properly saved
        logger.info(f"Task {task_id} execution completed. Success: {task_completed}")
        if task_id in tasks_status:
            logger.info(f"Final task status: {tasks_status[task_id].get('status', 'unknown')}")
            save_task_status(task_id)
        
        # Unregister task from task manager
        task_manager.unregister_task(task_id)

@app.route('/')
def index():
    """Serve the main web interface"""
    # Serve as static file to avoid Jinja2 template processing
    import os
    # Get absolute path to template
    current_dir = os.path.dirname(os.path.abspath(__file__))
    template_path = os.path.join(current_dir, '..', 'templates', 'index.html')
    template_path = os.path.abspath(template_path)
    
    with open(template_path, 'r', encoding='utf-8') as f:
        return f.read(), 200, {'Content-Type': 'text/html'}

@app.route('/api/create-task', methods=['POST'])
def create_task():
    """Create a new automated PR task"""
    data = request.json
    
    required_fields = ['devpod_name', 'github_repo', 'github_token', 'task_description']
    for field in required_fields:
        if field not in data:
            return jsonify({'error': f'Missing required field: {field}'}), 400
    
    task_id = f"task-{int(time.time())}"
    
    # Start task in background thread
    thread = threading.Thread(
        target=execute_remote_task,
        args=(task_id, data['devpod_name'], data['github_repo'], 
              data['github_token'], data['task_description'])
    )
    thread.daemon = False  # Keep thread running even if main process restarts
    thread.start()
    
    # Log thread information for monitoring
    logger.info(f"Started background task {task_id} in thread {thread.name}")
    
    # Register task with task manager
    task_manager.register_task(task_id, thread)
    
    return jsonify({'task_id': task_id})

@app.route('/api/task-status/<task_id>')
def get_task_status(task_id):
    """Get status of a specific task"""
    with tasks_lock:
        if task_id in tasks_status:
            # Make a copy to ensure all fields are included
            task_data = dict(tasks_status[task_id])
            task_data['task_id'] = task_id
            return jsonify(task_data)
        else:
            return jsonify({'error': 'Task not found'}), 404

@app.route('/api/tasks')
def list_tasks():
    """List all tasks from MongoDB"""
    try:
        # Get tasks from MongoDB instead of memory
        tasks = db.get_all_tasks(limit=100)
        
        # Add runtime status
        for task in tasks:
            task_id = task.get('task_id')
            if task_id and task_id in task_manager.active_tasks:
                thread_info = task_manager.active_tasks[task_id]
                task['is_running'] = thread_info['thread'].is_alive()
            else:
                task['is_running'] = False
        
        return jsonify(tasks)
    except Exception as e:
        logger.error(f"Failed to list tasks: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/tasks/by-repo')
def list_tasks_by_repo():
    """List tasks grouped by repository"""
    try:
        grouped_tasks = db.get_tasks_grouped_by_repo()
        return jsonify(grouped_tasks)
    except Exception as e:
        logger.error(f"Failed to get grouped tasks: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/tasks/repo/<path:github_repo>')
def list_tasks_for_repo(github_repo):
    """List tasks for a specific repository"""
    try:
        tasks = db.get_tasks_by_repo(github_repo, limit=50)
        
        # Add runtime status
        for task in tasks:
            task_id = task.get('task_id')
            if task_id and task_id in task_manager.active_tasks:
                thread_info = task_manager.active_tasks[task_id]
                task['is_running'] = thread_info['thread'].is_alive()
            else:
                task['is_running'] = False
        
        return jsonify(tasks)
    except Exception as e:
        logger.error(f"Failed to get tasks for repo {github_repo}: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/repository-stats')
def get_repository_stats():
    """Get statistics for all repositories"""
    try:
        stats = db.get_repository_stats()
        return jsonify(stats)
    except Exception as e:
        logger.error(f"Failed to get repository stats: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/dashboard')
def dashboard():
    """Get dashboard data - fast, non-blocking"""
    try:
        with tasks_lock:
            # Quick stats calculation
            total_tasks = len(tasks_status)
            completed_tasks = sum(1 for t in tasks_status.values() if t.get('status') == 'completed')
            failed_tasks = sum(1 for t in tasks_status.values() if t.get('status') == 'failed')
            running_tasks = sum(1 for t in tasks_status.values() if t.get('status') not in ['completed', 'failed', 'interrupted'])
            
            # Get recent tasks with their IDs (limit sensitive data)
            recent_tasks = []
            for task_id, task_data in sorted(tasks_status.items(), key=lambda x: x[1].get('created_at', ''), reverse=True)[:10]:
                # Create a safe copy without sensitive data
                safe_task = {
                    'task_id': task_id,
                    'status': task_data.get('status', 'unknown'),
                    'progress': task_data.get('progress', 0),
                    'created_at': task_data.get('created_at', ''),
                    'last_updated': task_data.get('last_updated', ''),
                    'devpod_name': task_data.get('devpod_name', ''),
                    'task_description': task_data.get('task_description', ''),
                    'github_repo': task_data.get('github_repo', ''),
                    'claude_status': task_data.get('claude_status', ''),
                    'app_url': task_data.get('app_url', ''),
                    'server_running': task_data.get('server_running', False),
                    'branch_name': task_data.get('branch_name', ''),
                    'logs': task_data.get('logs', [])[-5:]  # Only last 5 logs for dashboard
                }
                recent_tasks.append(safe_task)
            
            return jsonify({
                'total_tasks': total_tasks,
                'completed_tasks': completed_tasks,
                'failed_tasks': failed_tasks,
                'running_tasks': running_tasks,
                'recent_tasks': recent_tasks
            })
    except Exception as e:
        logger.error(f"Dashboard error: {e}")
        return jsonify({
            'total_tasks': 0,
            'completed_tasks': 0,
            'failed_tasks': 0,
            'running_tasks': 0,
            'recent_tasks': [],
            'error': str(e)
        })

@app.route('/api/task/<task_id>/continue', methods=['POST'])
def continue_task(task_id):
    """Continue a task that requires authentication"""
    with tasks_lock:
        if task_id not in tasks_status:
            return jsonify({'error': 'Task not found'}), 404
        
        if tasks_status[task_id].get('requires_authentication', False):
            tasks_status[task_id]['authentication_confirmed'] = True
            return jsonify({'status': 'ok', 'message': 'Task will continue'})
        else:
            return jsonify({'error': 'Task does not require authentication'}), 400

@app.route('/api/task/<task_id>/check-alive')
def check_task_alive(task_id):
    """Check if a task is still running"""
    is_alive = False
    thread_info = None
    
    if task_id in task_manager.active_tasks:
        info = task_manager.active_tasks[task_id]
        is_alive = info['thread'].is_alive()
        thread_info = {
            'thread_name': info['thread_name'],
            'started_at': info['started_at'],
            'is_alive': is_alive
        }
    
    # Get task status
    task_status = None
    with tasks_lock:
        if task_id in tasks_status:
            task_status = tasks_status[task_id].get('status')
    
    return jsonify({
        'task_id': task_id,
        'is_alive': is_alive,
        'thread_info': thread_info,
        'task_status': task_status
    })

@app.route('/api/task/<task_id>/commit', methods=['POST'])
def commit_task_changes(task_id):
    """Commit changes for a specific task"""
    data = request.json
    commit_message = data.get('commit_message', '')
    
    with tasks_lock:
        if task_id not in tasks_status:
            return jsonify({'error': 'Task not found'}), 404
        
        task_data = tasks_status[task_id]
        if not task_data.get('has_changes', False):
            return jsonify({'error': 'No changes to commit'}), 400
    
    devpod_name = task_data['devpod_name']
    github_repo = task_data['github_repo']
    repo_name = github_repo.split('/')[-1]
    
    try:
        pod_name = get_pod_name(devpod_name)
        
        # Use provided message or generate one
        if not commit_message:
            commit_message = f"Task: {task_data['task_description']}"
        
        # Add and commit changes
        add_log(task_id, f"Committing changes with message: {commit_message}")
        
        commit_cmds = [
            f'cd ~/{repo_name} && git add -A',
            f'cd ~/{repo_name} && git commit -m "{commit_message}"'
        ]
        
        for cmd in commit_cmds:
            result = exec_in_devpod(devpod_name, cmd, pod_name)
            if result.returncode != 0 and "nothing to commit" not in result.stdout:
                return jsonify({'error': f'Commit failed: {result.stderr}'}), 500
        
        add_log(task_id, "âœ… Changes committed successfully")
        
        # Update task status
        with tasks_lock:
            tasks_status[task_id]['is_committed'] = True
            tasks_status[task_id]['commit_message'] = commit_message
        save_task_status(task_id)
        
        return jsonify({'status': 'success', 'message': 'Changes committed'})
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/task/<task_id>/create-pr', methods=['POST'])
def create_pr_from_task(task_id):
    """Create a PR from task changes"""
    data = request.json
    
    with tasks_lock:
        if task_id not in tasks_status:
            return jsonify({'error': 'Task not found'}), 404
        
        task_data = tasks_status[task_id]
        if not task_data.get('is_committed', False) and task_data.get('has_changes', False):
            return jsonify({'error': 'Please commit changes first'}), 400
    
    devpod_name = task_data['devpod_name']
    github_repo = task_data['github_repo']
    github_token = data.get('github_token')
    if not github_token:
        return jsonify({'error': 'GitHub token is required for PR creation'}), 400
    pr_title = data.get('pr_title', f"Task: {task_data['task_description'][:50]}...")
    pr_body = data.get('pr_body', f"Automated task execution:\n\n{task_data['task_description']}")
    repo_name = github_repo.split('/')[-1]
    
    try:
        pod_name = get_pod_name(devpod_name)
        
        # Create a new branch from current state
        branch_name = f"auto-pr-{int(time.time())}"
        add_log(task_id, f"Creating new branch: {branch_name}")
        
        branch_cmds = [
            f'cd ~/{repo_name} && git checkout -b {branch_name}',
            f'cd ~/{repo_name} && git push origin {branch_name}'
        ]
        
        for cmd in branch_cmds:
            result = exec_in_devpod(devpod_name, cmd, pod_name)
            if result.returncode != 0:
                return jsonify({'error': f'Branch operation failed: {result.stderr}'}), 500
        
        add_log(task_id, "Branch created and pushed")
        
        # Create PR using the existing create_pr logic
        return create_pr_with_details(devpod_name, github_repo, github_token, 
                                     pr_title, pr_body, branch_name, pod_name)
        
    except Exception as e:
        return jsonify({'error': str(e)}), 500

def create_pr_with_details(devpod_name, github_repo, github_token, pr_title, pr_body, branch_name, pod_name):
    """Helper function to create PR with given details"""
    repo_name = github_repo.split('/')[-1]
    
    try:
        # Install gh CLI if needed
        gh_check = exec_in_devpod(devpod_name, 'which gh', pod_name)
        if not gh_check.stdout.strip():
            gh_install = 'curl -fsSL https://cli.github.com/packages/githubcli-archive-keyring.gpg | dd of=/usr/share/keyrings/githubcli-archive-keyring.gpg && echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/githubcli-archive-keyring.gpg] https://cli.github.com/packages stable main" | tee /etc/apt/sources.list.d/github-cli.list > /dev/null && apt update && apt install gh -y'
            exec_in_devpod(devpod_name, gh_install, pod_name)
        
        # Authenticate gh with token
        auth_cmd = f'echo {github_token} | gh auth login --with-token'
        exec_in_devpod(devpod_name, auth_cmd, pod_name)
        
        # Create PR
        pr_cmd = f'cd ~/{repo_name} && gh pr create --title "{pr_title}" --body "{pr_body}" --base main --head {branch_name}'
        pr_result = exec_in_devpod(devpod_name, pr_cmd, pod_name)
        
        if pr_result.returncode == 0:
            pr_url = pr_result.stdout.strip()
            return jsonify({'status': 'success', 'pr_url': pr_url})
        else:
            return jsonify({'error': f'Failed to create PR: {pr_result.stderr}'}), 500
            
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/create-pr', methods=['POST'])
def create_pr():
    """Legacy endpoint - Create a pull request for a DevPod"""
    data = request.json
    
    required_fields = ['devpod_name', 'github_repo', 'github_token', 'pr_title']
    for field in required_fields:
        if field not in data:
            return jsonify({'error': f'Missing required field: {field}'}), 400
    
    devpod_name = data['devpod_name']
    github_repo = data['github_repo']
    github_token = data['github_token']
    pr_title = data['pr_title']
    pr_body = data.get('pr_body', 'Pull request created by Remote Developer')
    
    try:
        # Get pod name
        pod_name = get_pod_name(devpod_name)
        repo_name = github_repo.split('/')[-1]
        
        # Get current branch
        branch_result = exec_in_devpod(devpod_name, f'cd ~/{repo_name} && git branch --show-current', pod_name)
        branch_name = branch_result.stdout.strip()
        
        if not branch_name or branch_name == 'main' or branch_name == 'master':
            return jsonify({'error': 'Please create and checkout a feature branch first'}), 400
        
        # Push branch
        push_cmd = f'cd ~/{repo_name} && git push origin {branch_name}'
        push_result = exec_in_devpod(devpod_name, push_cmd, pod_name)
        
        if push_result.returncode != 0:
            return jsonify({'error': f'Failed to push branch: {push_result.stderr}'}), 500
        
        # Install gh CLI if needed
        gh_check = exec_in_devpod(devpod_name, 'which gh', pod_name)
        if not gh_check.stdout.strip():
            gh_install = 'curl -fsSL https://cli.github.com/packages/githubcli-archive-keyring.gpg | dd of=/usr/share/keyrings/githubcli-archive-keyring.gpg && echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/githubcli-archive-keyring.gpg] https://cli.github.com/packages stable main" | tee /etc/apt/sources.list.d/github-cli.list > /dev/null && apt update && apt install gh -y'
            exec_in_devpod(devpod_name, gh_install, pod_name)
        
        # Authenticate gh with token
        auth_cmd = f'echo {github_token} | gh auth login --with-token'
        exec_in_devpod(devpod_name, auth_cmd, pod_name)
        
        # Create PR
        pr_cmd = f'cd ~/{repo_name} && gh pr create --title "{pr_title}" --body "{pr_body}" --base main --head {branch_name}'
        pr_result = exec_in_devpod(devpod_name, pr_cmd, pod_name)
        
        if pr_result.returncode == 0:
            pr_url = pr_result.stdout.strip()
            return jsonify({'status': 'success', 'pr_url': pr_url})
        else:
            return jsonify({'error': f'Failed to create PR: {pr_result.stderr}'}), 500
            
    except Exception as e:
        return jsonify({'error': str(e)}), 500

@app.route('/api/task/<task_id>/logs')
def get_task_logs(task_id):
    """Get all logs for a task from MongoDB"""
    try:
        logs = db.get_logs(task_id, limit=1000)
        return jsonify({
            'task_id': task_id,
            'logs': [log['message'] for log in logs],
            'total': len(logs)
        })
    except Exception as e:
        logger.error(f"Failed to get logs for task {task_id}: {e}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/task-logs/<task_id>/stream')
def stream_task_logs(task_id):
    """Stream task logs using Server-Sent Events"""
    def generate():
        # Create a queue for this client
        client_queue = queue.Queue()
        
        # Register this client for the task
        with log_streams_lock:
            if task_id not in log_streams:
                log_streams[task_id] = []
            log_streams[task_id].append(client_queue)
        
        try:
            # Send existing logs from MongoDB first
            existing_logs = db.get_logs(task_id, limit=100)
            for log in existing_logs:
                yield f"data: {json.dumps({'log': log['message']})}\n\n"
            
            # Stream new logs
            while True:
                try:
                    # Wait for new log with timeout
                    log = client_queue.get(timeout=30)
                    yield f"data: {json.dumps({'log': log})}\n\n"
                except queue.Empty:
                    # Send heartbeat to keep connection alive
                    yield f"data: {json.dumps({'heartbeat': True})}\n\n"
                    
                # Check if task is completed
                with tasks_lock:
                    if task_id in tasks_status:
                        status = tasks_status[task_id]['status']
                        claude_status = tasks_status[task_id].get('claude_status', '')
                        progress = tasks_status[task_id].get('progress', 0)
                        
                        # Send status update
                        yield f"data: {json.dumps({'status': status, 'claude_status': claude_status, 'progress': progress})}\n\n"
                        
                        if status in ['completed', 'failed']:
                            yield f"data: {json.dumps({'status': status, 'complete': True, 'final': True})}\n\n"
                            break
                            
        finally:
            # Unregister this client
            with log_streams_lock:
                if task_id in log_streams and client_queue in log_streams[task_id]:
                    log_streams[task_id].remove(client_queue)
                    if not log_streams[task_id]:
                        del log_streams[task_id]
    
    return Response(generate(), mimetype="text/event-stream")

if __name__ == '__main__':
    # Check for orphaned tasks from previous runs
    orphaned_tasks = task_manager.check_orphaned_tasks()
    if orphaned_tasks:
        logger.warning(f"Found {len(orphaned_tasks)} orphaned tasks from previous run:")
        for task in orphaned_tasks:
            logger.warning(f"  - {task['task_id']}: {task['status']} (last updated: {task['last_updated']})")
            # Mark orphaned tasks as interrupted
            if task['task_id'] in tasks_status:
                tasks_status[task['task_id']]['status'] = 'interrupted'
                tasks_status[task['task_id']]['error'] = 'Server was restarted while task was running'
                save_task_status(task['task_id'])
    
    # Start task monitor in background
    monitor_thread = threading.Thread(target=task_manager.monitor_tasks, daemon=True)
    monitor_thread.start()
    
    app.run(host='0.0.0.0', port=15001, debug=False, threaded=True)